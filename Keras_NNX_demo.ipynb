{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Self link: NA"
      ],
      "metadata": {
        "id": "kx1thMFoyNie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Guide to the Keras & Flax NNX Integration\n",
        "\n",
        "This tutorial will guide you through the integration of Keras with Flax's NNX (Neural Networks JAX) module system, demonstrating how it significantly enhances variable handling and opens up advanced training capabilities within the JAX ecosystem. Whether you love the simplicity of model.fit() or the fine-grained control of a custom training loop, this integration lets you have the best of both worlds. Let's dive in!"
      ],
      "metadata": {
        "id": "wU1y7jnJEx9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Keras and NNX Integration?\n",
        "\n",
        "Keras is known for its user-friendliness and high-level API, making deep learning accessible. JAX, on the other hand, provides high-performance numerical computation, especially suited for machine learning research due to its JIT compilation and automatic differentiation capabilities. NNX is Flax's functional module system built on JAX, offering explicit state management and powerful functional programming paradigms\n",
        "\n",
        "NNX is designed for simplicity. It is characterized by its Pythonic approach, where modules are standard Python classes, promoting ease of use and familiarity. NNX prioritizes user-friendliness and offers fine-grained control over JAX transformations through typed Variable collections\n",
        "\n",
        "The integration of Keras with NNX allows you to leverage the best of both worlds: the simplicity and modularity of Keras for model construction, combined with the power and explicit control of NNX and JAX for variable management and sophisticated training loops."
      ],
      "metadata": {
        "id": "HKzKaLIZFvj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started: Setting Up Your Environment"
      ],
      "metadata": {
        "id": "33iB5FgbGVHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXA8SQLhEngW",
        "outputId": "3b9e03e2-048e-41cc-b028-a55fbadeeb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Found existing installation: flax 0.10.6\n",
            "Uninstalling flax-0.10.6:\n",
            "  Successfully uninstalled flax-0.10.6\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.9/455.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q keras==3.11.1\n",
        "!pip uninstall -y flax\n",
        "!pip install -q flax==0.11.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enabling NNX Mode\n",
        "\n",
        "To activate the integration, we must set two environment variables before importing Keras. This tells Keras to use the JAX backend and switch to NNX as an opt in feature."
      ],
      "metadata": {
        "id": "__WJf7SxHJDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"]=\"jax\"\n",
        "os.environ[\"KERAS_NNX_ENABLED\"]=\"true\"\n",
        "from flax import nnx\n",
        "import keras\n",
        "import jax.numpy as jnp\n",
        "print(\"✅ Keras is now running on JAX with NNX enabled!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_zGLRAfG1gW",
        "outputId": "afc9092d-359f-46ae-b20f-38788081bb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.1 is installed, but it is not compatible with the installed jaxlib version 0.7.0, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Keras is now running on JAX with NNX enabled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Core Integration: Keras Variables in NNX\n",
        "\n",
        "The heart of this integration is the new keras.Variable, which is designed to be a native citizen of the Flax NNX ecosystem. This means you can mix Keras and NNX components freely, and NNX's tracing and state management tools will understand your Keras variables.\n",
        "Let's prove it. We'll create an nnx.Module that contains both a standard nnx.Linear layer and a keras.Variable."
      ],
      "metadata": {
        "id": "A_49uAf_HxD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Variable as KerasVariable\n",
        "\n",
        "class MyNnxModel(nnx.Module):\n",
        "  def __init__(self, rngs):\n",
        "    self.linear = nnx.Linear(2, 3, rngs=rngs)\n",
        "    self.custom_variable = KerasVariable(jnp.ones((1, 3)))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.linear(x) + self.custom_variable\n",
        "\n",
        "# Instantiate the model\n",
        "model = MyNnxModel(rngs=nnx.Rngs(0))\n",
        "\n",
        "# --- Verification ---\n",
        "# 1. Is the KerasVariable traced by NNX?\n",
        "print(f\"✅ Traced: {hasattr(model.custom_variable, '_trace_state')}\")\n",
        "\n",
        "# 2. Does NNX see the KerasVariable in the model's state?\n",
        "print(\"✅ Variables:\", nnx.variables(model))\n",
        "\n",
        "# 3. Can we access its value directly?\n",
        "print(\"✅ Value:\", model.custom_variable.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0O9msHVHn3V",
        "outputId": "81c5ebcd-8efe-4bec-bfd1-83968b6e9dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Traced: True\n",
            "✅ Variables: \u001b[38;2;79;201;177mState\u001b[0m\u001b[38;2;255;213;3m({\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n",
            "  \u001b[38;2;156;220;254m'custom_variable'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariable\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 3 (12 B)\u001b[0m\n",
            "    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray([[1., 1., 1.]], dtype=float32),\n",
            "    \u001b[38;2;156;220;254m_layout\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'None'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_name\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'variable'\"\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_path\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'variable'\"\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_shape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'(1, 3)'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_initializer\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'None'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_regularizer\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'None'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_constraint\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'None'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_trainable\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'True'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_autocast\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'True'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_aggregation\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'none'\"\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_synchronization\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'auto'\"\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_overwrite_with_gradient\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'False'\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_dtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'float32'\"\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m_ndim\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'2'\u001b[0m\n",
            "  \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "  \u001b[38;2;156;220;254m'linear'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n",
            "    \u001b[38;2;156;220;254m'bias'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 3 (12 B)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray([0., 0., 0.], dtype=float32)\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "    \u001b[38;2;156;220;254m'kernel'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 6 (24 B)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray([[ 0.7567299 , -0.68570054, -0.5688337 ],\n",
            "             [-0.8757605 , -0.65994275,  0.44935086]], dtype=float32)\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\n",
            "  \u001b[38;2;255;213;3m}\u001b[0m\n",
            "\u001b[38;2;255;213;3m})\u001b[0m\n",
            "✅ Value: [[1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this shows:\n",
        "The KerasVariable is successfully traced by NNX, just like any native nnx.Variable.\n",
        "The nnx.variables() function correctly identifies and lists our custom_variable as part of the model's state.\n",
        "This confirms that Keras state and NNX state can live together in perfect harmony."
      ],
      "metadata": {
        "id": "r5wzS5mvIRzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Best of Both Worlds: Training Workflows\n",
        "\n",
        "Now for the exciting part: training models. This integration unlocks two powerful workflows.\n"
      ],
      "metadata": {
        "id": "nw0YwXNZIVe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow 1: The Classic Keras Experience (model.fit)"
      ],
      "metadata": {
        "id": "LbJqrCGTIdfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. Create a Keras Model ---\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(units=1, input_shape=(10,), name=\"my_dense_layer\")\n",
        "])\n",
        "\n",
        "print(\"--- Initial Model Weights ---\")\n",
        "initial_weights = model.get_weights()\n",
        "print(f\"Initial Kernel: {initial_weights[0].T}\") # .T for better display\n",
        "print(f\"Initial Bias: {initial_weights[1]}\")\n",
        "\n",
        "# --- 2. Create Dummy Data ---\n",
        "X_dummy = np.random.rand(100, 10)\n",
        "y_dummy = np.random.rand(100, 1)\n",
        "\n",
        "# --- 3. Compile and Fit ---\n",
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss='mean_squared_error')\n",
        "\n",
        "print(\"\\n--- Training with model.fit() ---\")\n",
        "history = model.fit(X_dummy, y_dummy, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# --- 4. Verify a change ---\n",
        "print(\"\\n--- Weights After Training ---\")\n",
        "updated_weights = model.get_weights()\n",
        "print(f\"Updated Kernel: {updated_weights[0].T}\")\n",
        "print(f\"Updated Bias: {updated_weights[1]}\")\n",
        "\n",
        "# Verification\n",
        "if not np.array_equal(initial_weights[1], updated_weights[1]):\n",
        "    print(\"\\n✅ SUCCESS: Model variables were updated during training.\")\n",
        "else:\n",
        "    print(\"\\n❌ FAILURE: Model variables were not updated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj0kyYz6IWvo",
        "outputId": "89d6fc65-fd10-493d-ee8e-38e222cc9266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Model Weights ---\n",
            "Initial Kernel: [[-0.73280704 -0.48195577 -0.15528442 -0.6698463   0.32282797 -0.38383538\n",
            "  -0.16966783 -0.5582647  -0.49732223 -0.37509474]]\n",
            "Initial Bias: [0.]\n",
            "\n",
            "--- Training with model.fit() ---\n",
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step - loss: 4.8358\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7676  \n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5910 \n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8973 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5531\n",
            "\n",
            "--- Weights After Training ---\n",
            "Updated Kernel: [[-0.4517825  -0.19075629  0.09772788 -0.37026355  0.57015383 -0.14341867\n",
            "   0.08303216 -0.3223103  -0.24230698 -0.12036043]]\n",
            "Updated Bias: [0.511793]\n",
            "\n",
            "✅ SUCCESS: Model variables were updated during training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, your existing Keras code works out-of-the-box, giving you a high-level, productive experience powered by JAX and NNX under the hood."
      ],
      "metadata": {
        "id": "33Pj3vthI-D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow 2: The Power of NNX: Custom Training Loops\n",
        "\n",
        "For maximum flexibility, you can treat any Keras layer or model as an nnx.Module and write your own training loop using libraries like Optax.\n",
        "This is perfect when you need fine-grained control over the gradient and update process."
      ],
      "metadata": {
        "id": "qJD24R7pI6ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import optax\n",
        "\n",
        "X = np.linspace(-jnp.pi, jnp.pi, 100)[:, None]\n",
        "Y = 0.8 * X + 0.1 + np.random.normal(0, 0.1, size=X.shape)\n",
        "\n",
        "class MySimpleKerasModel(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Define the layers of your model\n",
        "        self.dense_layer = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define the forward pass\n",
        "        # The 'inputs' argument will receive the input tensor when the model is called\n",
        "        return self.dense_layer(inputs)\n",
        "\n",
        "model = MySimpleKerasModel()\n",
        "model(X)\n",
        "\n",
        "tx = optax.sgd(1e-3)\n",
        "trainable_var = nnx.All(keras.Variable, lambda path, x: getattr(x, '_trainable', False))\n",
        "optimizer = nnx.Optimizer(model, tx, wrt=trainable_var)\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, batch):\n",
        "  x, y = batch\n",
        "\n",
        "  def loss_fn(model_):\n",
        "    y_pred = model_(x)\n",
        "    return jnp.mean((y - y_pred) ** 2)\n",
        "\n",
        "  diff_state = nnx.DiffState(0, trainable_var)\n",
        "  grads = nnx.grad(loss_fn, argnums=diff_state)(model)\n",
        "  optimizer.update(model, grads)\n",
        "\n",
        "@nnx.jit\n",
        "def test_step(model, batch):\n",
        "  x, y = batch\n",
        "  y_pred = model(x)\n",
        "  loss = jnp.mean((y - y_pred) ** 2)\n",
        "  return {'loss': loss}\n",
        "\n",
        "\n",
        "def dataset(batch_size=10):\n",
        "  while True:\n",
        "    idx = np.random.choice(len(X), size=batch_size)\n",
        "    yield X[idx], Y[idx]\n",
        "\n",
        "for step, batch in enumerate(dataset()):\n",
        "  train_step(model, optimizer, batch)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    logs = test_step(model, (X, Y))\n",
        "    print(f\"step: {step}, loss: {logs['loss']}\")\n",
        "\n",
        "  if step >= 500:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfb59OlBJCpJ",
        "outputId": "aab511d5-cf15-4c48-b1c9-46b2659a41d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 0.019710825756192207\n",
            "step: 100, loss: 0.016170326620340347\n",
            "step: 200, loss: 0.013851000927388668\n",
            "step: 300, loss: 0.01240807306021452\n",
            "step: 400, loss: 0.011362794786691666\n",
            "step: 500, loss: 0.010727993212640285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example shows how a keras model object is seamlessly passed to nnx.Optimizer and differentiated by nnx.grad. This composition allows you to integrate Keras components into sophisticated JAX/NNX workflows. This approach also works perfectly with sequential, functional, subclassed keras models are even just layers."
      ],
      "metadata": {
        "id": "8gEW169_LbT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Saving and Loading\n",
        "\n",
        "Your investment in the Keras ecosystem is safe. Standard features like model serialization work exactly as you'd expect."
      ],
      "metadata": {
        "id": "sKif8DN-LqIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple model\n",
        "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=(10,))])\n",
        "dummy_input = np.random.rand(1, 10)\n",
        "\n",
        "# Test call\n",
        "print(\"Original model output:\", model(dummy_input))\n",
        "\n",
        "# Save and load\n",
        "model.save('my_nnx_model.keras')\n",
        "restored_model = keras.models.load_model('my_nnx_model.keras')\n",
        "\n",
        "print(\"Restored model output:\", restored_model(dummy_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVHmBpnJL-vl",
        "outputId": "f3e64446-48b3-431d-bc97-17918679a62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model output: [[0.4851133]]\n",
            "Restored model output: [[0.4851133]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-World Application: Training Gemma\n",
        "\n",
        "Before trying out this KerasHub model, please make sure you have set up your Kaggle credentials in colab secrets. The colab pulls in `KAGGLE_KEY` and `KAGGLE_USERNAME` to authenticate and download the models."
      ],
      "metadata": {
        "id": "D9nqTX_xMNe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_hub\n",
        "\n",
        "# Set a float16 policy for memory efficiency\n",
        "keras.config.set_dtype_policy(\"float16\")\n",
        "\n",
        "# Load Gemma from KerasHub\n",
        "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_2b_en\")\n",
        "\n",
        "# --- 1. Inference / Generation ---\n",
        "print(\"--- Gemma Generation ---\")\n",
        "output = gemma_lm.generate(\"Keras is a\", max_length=30)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WwZP__IGMOvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Fine-tuning ---\n",
        "print(\"\\n--- Gemma Fine-tuning ---\")\n",
        "# Dummy data for demonstration\n",
        "features = np.array([\"The quick brown fox jumped.\", \"I forgot my homework.\"])\n",
        "# The model.fit() API works seamlessly!\n",
        "gemma_lm.fit(x=features, batch_size=2)\n",
        "print(\"\\n✅ Gemma fine-tuning step completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzdOtjcNbydZ",
        "outputId": "33e90265-6159-4bbb-fcbb-f888f53134bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Gemma Fine-tuning ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 114s/step - loss: 0.1040 - sparse_categorical_accuracy: 0.1538\n",
            "\n",
            "✅ Gemma fine-tuning step completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The Keras-NNX integration represents a significant step forward, offering a unified framework for both rapid prototyping and high-performance, customizable research. You can now:\n",
        "Use familiar Keras APIs (Sequential, Model, fit, save) on a JAX backend.\n",
        "Integrate Keras layers and models directly into Flax NNX modules and training loops.Integrate keras code/model with NNX ecosytem like Qwix, Tunix, etc.\n",
        "Leverage the entire JAX ecosystem (e.g., nnx.jit, optax) with your Keras models.\n",
        "Seamlessly work with large models from KerasHub."
      ],
      "metadata": {
        "id": "ySsCE_FHOArO"
      }
    }
  ]
}
